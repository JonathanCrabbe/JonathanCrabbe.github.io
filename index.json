[{"authors":null,"categories":null,"content":"I am currently working towards my PhD thesis in the Applied Mathematics Department at the University of Cambridge. In this stimulating environment, I am learning to become a well-rounded Machine Learning researcher.\nWe are gradually entering in a phase where humans will increasingly interact with generative and predictive AIs, hence forming human-AI teams. I see an immense potential in these teams to approach cutting-edge scientific and medical problems. My research focuses on making these teams more efficient by improving the information flow between complex ML models and human users. This touches upon various subjects of the AI literature, including ML Interpretability, Robust ML and Data-Centric AI. In some sense, my goal is to build this microscope that would allow human beings to look inside a machine learning model. Through the interface of this microscope, human beings can rigorously validate ML models, extract knowledge from them and learn to use these models more efficiently.\nDownload my resumé.\n","date":1698192000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1698192000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am currently working towards my PhD thesis in the Applied Mathematics Department at the University of Cambridge. In this stimulating environment, I am learning to become a well-rounded Machine Learning researcher.","tags":null,"title":"Jonathan Crabbé","type":"authors"},{"authors":["Jonathan Crabbé","Pau Rodríguez","Vaishaal Shankar","Luca Zappella","Arno Blaas"],"categories":null,"content":"","date":1698192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698192000,"objectID":"56ce532b09f235d665fa3a7642f7c5d8","permalink":"https://jonathancrabbe.github.io/publication/eri/","publishdate":"2023-10-25T00:00:00Z","relpermalink":"/publication/eri/","section":"publication","summary":"We investigate what makes multimodal models that show good robustness with respect to natural distribution shifts (e.g., zero-shot CLIP) different from models with lower robustness using interpretability.","tags":["Robustness","Deep Learning","Multimodal","CLIP","Interpretability","Explainability","Generalization","Polysemanticity","Natural Distribution Shifts","Outlier Features","Privileged Directions","Model Pruning","Effective Robustness"],"title":"Robust multimodal models have outlier features and encode more concepts","type":"publication"},{"authors":["Arthur Vandenhoeke","Lennert Antson","Guillem Ballesteros","Jonathan Crabbé","Michal Shimoni"],"categories":null,"content":"","date":1698105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698105600,"objectID":"b369b7a47ef4c87011f08c74c5660394","permalink":"https://jonathancrabbe.github.io/publication/absorption/","publishdate":"2023-10-25T00:00:00Z","relpermalink":"/publication/absorption/","section":"publication","summary":"Over the past decade, Deep Learning (DL) models have proven to be efficient at classifying remotely sensed Earth Observation (EO) hyperspectral imaging (HSI) data. Those models show state-of-the-art performances across various bench-marked data sets by extracting abstract spatial-spectral features using 2D and 3D convolutions. However, the black-box nature of DL models hinders explanation, limits trust, and underscores the need for profound insights beyond raw performance metrics. In this contribution, we implement a simple yet powerful mechanism for the explainability of DL-based absorption features using an axiomatic approach called Integrated Gradients, and showcase how such an approach can be used to evaluate the relevance of a network’s decisions, and compare network sensitivities when trained using single and dual sensor data.","tags":["Computer Vision","Deep Learning","Interpretability","Explainability"],"title":"Explaining the Absorption Features of Deep Learning Hyperspectral Classification Models","type":"publication"},{"authors":["Nabeel Seedat","Jonathan Crabbé","Zhaozhi Qian","Mihaela van der Schaar"],"categories":null,"content":"","date":1697673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697673600,"objectID":"448085d67b1281313f6b84f477734738","permalink":"https://jonathancrabbe.github.io/publication/triage/","publishdate":"2023-10-19T00:00:00Z","relpermalink":"/publication/triage/","section":"publication","summary":"TRIAGE provides systematic data characterization for regression settings; compatible with a variety of regressors.","tags":["Data-Centric AI","Robust Machine Learning","Regression","Conformal Uncertainty"],"title":"TRIAGE: Characterizing and auditing training data for improved regression","type":"publication"},{"authors":["Jonathan Crabbé","Mihaela van der Schaar"],"categories":null,"content":"","date":1681344000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1681344000,"objectID":"6a168282eb9005cdf81e8b775d6f4755","permalink":"https://jonathancrabbe.github.io/publication/robustxai/","publishdate":"2023-04-13T00:00:00Z","relpermalink":"/publication/robustxai/","section":"publication","summary":"We assess the robustness of various interpretability methods by measuring how their explanations change when applying symmetries of the model to the input features.","tags":["Robustness","Geometric","Deep Learning","Equivariance","Invariance","Interpretability","Explainability"],"title":"Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance","type":"publication"},{"authors":["Alan Jeffares","Tennison Liu","Jonathan Crabbé","Fergus Imrie","Mihaela van der Schaar"],"categories":null,"content":"","date":1678320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678320000,"objectID":"b17701a5db184559aebcad83284a64d7","permalink":"https://jonathancrabbe.github.io/publication/tangos/","publishdate":"2023-03-09T00:00:00Z","relpermalink":"/publication/tangos/","section":"publication","summary":"We introduce TANGOS, a regularization method that orthogonalizes the gradient attribution of neurons to improve the generalization of deep neural networks on tabular data.","tags":["Latent Representation","Tabular Data","Regularization","Deep Learning"],"title":"TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization","type":"publication"},{"authors":["Alan Jeffares","Tennison Liu","Jonathan Crabbé","Mihaela van der Schaar"],"categories":null,"content":"","date":1674691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674691200,"objectID":"95b6efe402aaad03e17d4cb9b48d5e12","permalink":"https://jonathancrabbe.github.io/publication/joint_training/","publishdate":"2023-01-26T00:00:00Z","relpermalink":"/publication/joint_training/","section":"publication","summary":"Training deep ensembles via a shared objective results in degenerate behavior.","tags":["Robustness","Deep Learning","Deep Ensembles","Optimization"],"title":"Joint Training of Deep Ensembles Fails Due to Learner Collusion","type":"publication"},{"authors":["Jonathan Crabbé","Mihaela van der Schaar"],"categories":null,"content":"","date":1663200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663200000,"objectID":"db7e50f563416ea82f946c2d01a3ea1b","permalink":"https://jonathancrabbe.github.io/publication/car/","publishdate":"2022-09-15T00:00:00Z","relpermalink":"/publication/car/","section":"publication","summary":"We extend existing feature and example importance methods to unsupervised learning.","tags":["Explainability","Interpretability","Latent Representation","Feature Importance","Concept-Based Explanation","Unsupervised Learning","Deep Learning"],"title":"Concept Activation Regions: A Generalized Framework For Concept-Based Explanations","type":"publication"},{"authors":["Nabeel Seedat","Jonathan Crabbé","Ioana Bica","Mihaela van der Schaar"],"categories":null,"content":"","date":1663200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663200000,"objectID":"8b938176d5699f09c3acb38e5998c3b5","permalink":"https://jonathancrabbe.github.io/publication/data_iq/","publishdate":"2022-09-15T00:00:00Z","relpermalink":"/publication/data_iq/","section":"publication","summary":"We introduce Data-IQ, a data-centric framework to identify ambiguous examples.","tags":["Data-Centric AI","Robust Machine Learning","Latent Representation","Aleatoric Uncertainty","Uncertainty","Ta6bular Data"],"title":"Data-IQ: Characterizing subgroups with heterogeneous outcomes in tabular data","type":"publication"},{"authors":[],"categories":null,"content":"Label-Free Explainability. The timing is available on this page.\nData-SUITE. The timing is available on this page.\n","date":1658016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658016000,"objectID":"6432d3ee7f805ca6e6ea1dbe06025427","permalink":"https://jonathancrabbe.github.io/talk/2-spotlight-presentations-at-icml-2022/","publishdate":"2022-06-08T00:00:00Z","relpermalink":"/talk/2-spotlight-presentations-at-icml-2022/","section":"event","summary":"Presentation of two accepted papers at the main conference.","tags":[],"title":"2 Spotlight Presentations at ICML 2022","type":"event"},{"authors":["Jonathan Crabbé","Alicia Curth","Ioana Bica","Mihaela van der Schaar"],"categories":null,"content":"","date":1655337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655337600,"objectID":"a8cf1596b983ad99e4d4a459700ba028","permalink":"https://jonathancrabbe.github.io/publication/iterpretability/","publishdate":"2022-06-16T00:00:00Z","relpermalink":"/publication/iterpretability/","section":"publication","summary":"We benchmark treatment effect models with interpretability tools.","tags":["Interpretability","Explainability","Robust Machine Learning","Treatment Effect","Medicine"],"title":"Benchmarking Heterogeneous Treatment Effect Models through the Lens of Interpretability","type":"publication"},{"authors":["Jonathan Crabbé"],"categories":["Conferences"],"content":"I’m extremely excited to present 2 works at ICML 2022 🎉 They touch on various subjects ranging from explainable AI to robust and data-centric machine learning. Unfortunately, I will be unable to attend the conference in person. Hence, do not miss the opportunity to interact with me during the online sessions. I’m always enthusiastic to engage and discuss these fascinating subjects 😄 I will post the relevant practical information on this page once they are available. Stay tuned 📻\nLabel-Free Explainability Label-Free Explainability is a new framework to extend feature and example importance methods to the unsupervised setting. If you have always dreamed to use SHAP or Influence Functions to explain the representations of your brand-new encoder, this paper is for you!\nData-SUITE Data-SUITE is a data-centric method to flag incongruous examples at inference time. If you want to avoid using your machine learning model on data that differs too much from the training data, you might want to take a look!\n","date":1654646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654646400,"objectID":"5e2445d214bacc100ff8c58b03c07527","permalink":"https://jonathancrabbe.github.io/post/icml2022_two_papers/","publishdate":"2022-06-08T00:00:00Z","relpermalink":"/post/icml2022_two_papers/","section":"post","summary":"Label-Free Explainability and Data-SUITE have been accepted at ICML 2022. These are two new methods that machine learning interpretability and robustness. I provide a quick summary of the methods along with the relevant links.","tags":["Academic","Conference","Publication","ICML"],"title":"2 Papers Accepted at ICML 2022","type":"post"},{"authors":["Jonathan Crabbé","Mihaela van der Schaar"],"categories":null,"content":"","date":1646265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646265600,"objectID":"ad312c3048c03ebab94e17cd1ce32646","permalink":"https://jonathancrabbe.github.io/publication/lfxai/","publishdate":"2022-03-03T00:00:00Z","relpermalink":"/publication/lfxai/","section":"publication","summary":"We extend existing feature and example importance methods to unsupervised learning.","tags":["Explainability","Interpretability","Latent Representation","Feature Importance","Example Importance","Influential Examples","Unsupervised Learning","Self-Supervised Learning","Deep Learning","Autoencoder","Variational Autoencoder","Generative AI"],"title":"Label-Free Explainability for Unsupervised Models","type":"publication"},{"authors":null,"categories":null,"content":"How can we make sure that machines learn a good representation of the data? If labelling the data is a typical solution, it is often costly and time-consuming. Models such as neural networks typically require thousands to millions of examples to solve a task. Is it possible to learn good representations of the data without having to label each one of these examples?\nRepresentation Learning provides interesting solutions through semi-supervised and self-supervised learning. With these techniques, it becomes increasingly realistic to solve complex tasks with few labelled examples. In some cases, we might want these representations to be understandable by human users. This induces a significant overlap with Explainable Artificial Intelligence.\n","date":1646265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646265600,"objectID":"5cf2a6b3cb4600f326ad0b3f40462d04","permalink":"https://jonathancrabbe.github.io/project/repl/","publishdate":"2022-03-03T00:00:00Z","relpermalink":"/project/repl/","section":"project","summary":"Learning realistic representations of the world.","tags":["Machine learning","Deep learning","Unsupervised Learning","Semi-Supervised Learning","Self-Supervised Learning","Transfer Learning","Few Shot Learning","Generative AI"],"title":"Representation Learning","type":"project"},{"authors":["Nabeel Seedat","Jonathan Crabbé","Mihaela van der Schaar"],"categories":null,"content":"","date":1645056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645056000,"objectID":"81a5cc0ad3bf11a2b81280670a139479","permalink":"https://jonathancrabbe.github.io/publication/data_suite/","publishdate":"2022-02-17T00:00:00Z","relpermalink":"/publication/data_suite/","section":"publication","summary":"We introduce Data-SUITE, a data-centric framework to identify incongruous examples.","tags":["Data-Centric AI","Robust Machine Learning","Latent Representation","Copula","Conformal Uncertainty"],"title":"Data-SUITE: Data-centric identification of in-distribution incongruous examples","type":"publication"},{"authors":null,"categories":null,"content":"How can we make a machine learning model reliable? If generalization to unseen data is undoubtedly necessary, it is rarely sufficient. Models such as neural networks typically involve millions of operations to turn their input data into a prediction. This complexity permits to solve hard problems like computer vision and protein structure prediction. However, these complex models tend to exhibit unpredictable behaviours, such as the sensitivity to adversarial perturbations and spurious correlations . When models penetrate critical areas such as medicine, finance and the criminal justice system, this unpredictability is highly problematic.\nRobust machine learning tackles this problem by providing tools to identify and fix the weaknesses of a model. This includes (but is not limited to) understanding the model’s failure modes, quantifying the model’s uncertainty and teaching a model to be robust with respect to noise/adversarial perturbations. With the development of Data-Centric AI, I believe that many new and exciting things can be done in this area.\n","date":1645056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645056000,"objectID":"2e1225c6ffc0839c600af70c4057f45a","permalink":"https://jonathancrabbe.github.io/project/rml/","publishdate":"2022-02-17T00:00:00Z","relpermalink":"/project/rml/","section":"project","summary":"Making sure that machines are doing what they are supposed to do.","tags":["Machine learning","Deep learning","Robustness","Reliability","Safety"],"title":"Robust Machine Learning","type":"project"},{"authors":["Hao Sun","Boris van Breugel","Jonathan Crabbé","Nabeel Seedat","Mihaela van der Schaar"],"categories":null,"content":"","date":1644451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644451200,"objectID":"90bee534475368785f2ecc842041e065","permalink":"https://jonathancrabbe.github.io/publication/daux/","publishdate":"2022-02-10T00:00:00Z","relpermalink":"/publication/daux/","section":"publication","summary":"We introduce DAUX, an interpretability framework to interpret model uncertainty.","tags":["Interpretability","Explainability","Robust Machine Learning","Latent Representation","Uncertainty"],"title":"Latent Density Models for Uncertainty Categorization","type":"publication"},{"authors":["Jonathan Crabbé","Zhaozhi Qian","Fergus Imrie","Mihaela van der Schaar"],"categories":null,"content":"","date":1637107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637107200,"objectID":"6b58acdbae782e99fd5e825a6a7d833c","permalink":"https://jonathancrabbe.github.io/publication/simplex/","publishdate":"2021-11-17T00:00:00Z","relpermalink":"/publication/simplex/","section":"publication","summary":"We introduce SimplEx, a case-based reasoning explanation method that permits to decompose latent representations with a corpus of example.","tags":["Explainability","Interpretability","Corpus","Latent Representation","Feature Importance","Example Importance","Influential Examples","Case-Based Reasoning","Deep Learning"],"title":"Explaining Latent Representations with a Corpus of Examples","type":"publication"},{"authors":["Jonathan Crabbé","Mihaela van der Schaar"],"categories":null,"content":"","date":1623196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623196800,"objectID":"b428aa361fdd98ce871ad7675c40fb49","permalink":"https://jonathancrabbe.github.io/publication/dynamask/","publishdate":"2021-06-09T00:00:00Z","relpermalink":"/publication/dynamask/","section":"publication","summary":"We introduce Dynamask, a perturbation-based feature importance method to explain the predictions of time series models.","tags":["Explainability","Interpretability","Time Series","Masks","Feature Importance","Machine Learning","Deep Learning"],"title":"Explaining Time Series Predictions with Dynamic Masks","type":"publication"},{"authors":["Jonathan Crabbé","Yao Zhang","William R. Zame","Mihaela van der Schaar"],"categories":null,"content":"","date":1605571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605571200,"objectID":"0bcdc340004444872ecc162ff96ab139","permalink":"https://jonathancrabbe.github.io/publication/symbolic_pursuit/","publishdate":"2020-11-17T00:00:00Z","relpermalink":"/publication/symbolic_pursuit/","section":"publication","summary":"We introduce Symbolic Pursuit, a new method for symbolic regression based on Meijer G-functions and the projection pursuit algorithm.","tags":["Symbolic Regression","Interpretability","Meijer G-Function","Projection Pursuit","Machine Learning","Deep Learning"],"title":"Learning outside the black-box: the pursuit of interpretable models","type":"publication"},{"authors":null,"categories":null,"content":"How can we make a machine learning model convincing? If accuracy is undoubtedly necessary, it is rarely sufficient. Models such as neural networks typically involve millions of operations to turn their input data into a prediction. This complexity permits to accurately solve hard problems like computer vision and protein structure prediction. However, this accuracy comes at the expense of interpretability: these complex models appear as black boxes for human users. When models penetrate critical areas such as medicine, finance and the criminal justice system, their black-box nature appears as a major issue. An important question follows: is it possible to explain the predictions of complex machine-learning models?\nExplainable AI tackles this question by providing an interface between complex models and human users. To illustrate, let us consider the example of a medical machine learning model that recommends a treatment for a patient. By using post-hoc explainability, we can answer crucial questions such as “What part of this patient’s data motivates the model’s recommendation?” or “Are there similar patients previously seen by the model for which this treatment worked?”. In a setting where human knowledge is available (e.g. computer vision), this type of information is crucial to validate/debug the model. In a setting where little human knowledge (e.g. scientific discovery), this type of information permits to extract knowledge from the model.\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"07f8e974c92fc49679f5a434ce1d7cbe","permalink":"https://jonathancrabbe.github.io/project/xai/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/project/xai/","section":"project","summary":"Creating an interface between machine learning models and human beings.","tags":["Machine learning","Deep learning","Transparency","Explainability","Interpretability"],"title":"Interpretable Machine Learning","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://jonathancrabbe.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]