[{"authors":null,"categories":null,"content":"I am currently working towards my PhD thesis in the van der Schaar lab, a leading machine learning lab from the University of Cambridge led by Mihaela van der Schaar. In this stimulating environment, I am learning to become a well-rounded machine learning researcher.\nMy research focuses on Explainable AI, with a special focus on explaining the latent representations that are involved in state-of-the-art machine learning models. I like to think of Explainable AI as a microscope that allows us to look inside a machine learning model, hence providing an interface between machine learning models and human beings. With this interface, human beings can extract knowledge from machine-learning models, which is challenging due to the inherent complexity of those models.\nDownload my resum√©.\n","date":1654646400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1654646400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am currently working towards my PhD thesis in the van der Schaar lab, a leading machine learning lab from the University of Cambridge led by Mihaela van der Schaar. In this stimulating environment, I am learning to become a well-rounded machine learning researcher.","tags":null,"title":"Jonathan Crabb√©","type":"authors"},{"authors":[],"categories":null,"content":"üöß This page will be updated when the full ICML 2022 schedule is available\nLabel-Free Explainability The timing should be posted on this page.\nData-SUITE The timing should be posted on this page.\n","date":1658016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658016000,"objectID":"6432d3ee7f805ca6e6ea1dbe06025427","permalink":"https://jonathancrabbe.github.io/talk/2-spotlight-presentations-at-icml-2022/","publishdate":"2022-06-08T00:00:00Z","relpermalink":"/talk/2-spotlight-presentations-at-icml-2022/","section":"event","summary":"Presentation of two accepted papers at the main conference.","tags":[],"title":"2 Spotlight Presentations at ICML 2022","type":"event"},{"authors":["Jonathan Crabb√©"],"categories":["Conferences"],"content":"I‚Äôm extremely excited to present 2 works at ICML 2022 üéâ They touch on various subjects ranging from explainable AI to robust and data-centric machine learning. Unfortunately, I will be unable to attend the conference in person. Hence, do not miss the opportunity to interact with me during the online sessions. I‚Äôm always enthusiastic to engage and discuss these fascinating subjects üòÑ I will post the relevant practical information on this page once they are available. Stay tuned üìª\nLabel-Free Explainability Label-Free Explainability is a new framework to extend feature and example importance methods to the unsupervised setting. If you have always dreamed to use SHAP or Influence Functions to explain the representations of your brand-new encoder, this paper is for you!\nData-SUITE Data-SUITE is a data-centric method to flag incongruous examples at inference time. If you want to avoid using your machine learning model on data that differs too much from the training data, you might want to take a look!\n","date":1654646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654646400,"objectID":"5e2445d214bacc100ff8c58b03c07527","permalink":"https://jonathancrabbe.github.io/post/icml2022_two_papers/","publishdate":"2022-06-08T00:00:00Z","relpermalink":"/post/icml2022_two_papers/","section":"post","summary":"Label-Free Explainability and Data-SUITE have been accepted at ICML 2022. These are two new methods that machine learning interpretability and robustness. I provide a quick summary of the methods along with the relevant links.","tags":["Academic","Conference","Publication","ICML"],"title":"2 Papers Accepted at ICML 2022","type":"post"},{"authors":["Jonathan Crabb√©","Mihaela van der Schaar"],"categories":null,"content":"","date":1646265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646265600,"objectID":"ad312c3048c03ebab94e17cd1ce32646","permalink":"https://jonathancrabbe.github.io/publication/lfxai/","publishdate":"2022-03-03T00:00:00Z","relpermalink":"/publication/lfxai/","section":"publication","summary":"We extend existing feature and example importance methods to unsupervised learning.","tags":["Explainability","Interpretability","Latent Representation","Feature Importance","Example Importance","Influential Examples","Unsupervised Learning","Self-Supervised Learning","Deep Learning","Autoencoder","Variational Autoencoder"],"title":"Label-Free Explainability for Unsupervised Models","type":"publication"},{"authors":null,"categories":null,"content":"How can we make sure that machines learn a good representation of the data? If labelling the data is a typical solution, it is often costly and time-consuming. Models such as neural networks typically require thousands to millions of examples to solve a task. Is it possible to learn good representations of the data without having to label each one of these examples?\nRepresentation Learning provides interesting solutions through semi-supervised and self-supervised learning. With these techniques, it becomes increasingly realistic to solve complex tasks with few labelled examples. In some cases, we might want these representations to be understandable by human users. This induces a significant overlap with Explainable Artificial Intelligence.\n","date":1646265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646265600,"objectID":"5cf2a6b3cb4600f326ad0b3f40462d04","permalink":"https://jonathancrabbe.github.io/project/repl/","publishdate":"2022-03-03T00:00:00Z","relpermalink":"/project/repl/","section":"project","summary":"Learning realistic representations of the world.","tags":["Machine learning","Deep learning","Unsupervised Learning","Semi-Supervised Learning","Self-Supervised Learning","Transfer Learning","Few Shot Learning"],"title":"Representation Learning","type":"project"},{"authors":["Nabeel Seedat","Jonathan Crabb√©","Mihaela van der Schaar"],"categories":null,"content":"","date":1645056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645056000,"objectID":"81a5cc0ad3bf11a2b81280670a139479","permalink":"https://jonathancrabbe.github.io/publication/data_suite/","publishdate":"2022-02-17T00:00:00Z","relpermalink":"/publication/data_suite/","section":"publication","summary":"We introduce Data-SUITE, a data-centric framework to identify incongruous examples.","tags":["Data-Centric AI","Robust Machine Learning","Latent Representation","Copula","Conformal Uncertainty"],"title":"Data-SUITE: Data-centric identification of in-distribution incongruous examples","type":"publication"},{"authors":null,"categories":null,"content":"How can we make a machine learning model reliable? If generalization to unseen data is undoubtedly necessary, it is rarely sufficient. Models such as neural networks typically involve millions of operations to turn their input data into a prediction. This complexity permits to solve hard problems like computer vision and protein structure prediction. However, these complex models tend to exhibit unpredictable behaviours, such as the sensitivity to adversarial perturbations and spurious correlations . When models penetrate critical areas such as medicine, finance and the criminal justice system, this unpredictability is highly problematic.\nRobust machine learning tackles this problem by providing tools to identify and fix the weaknesses of a model. This includes (but is not limited to) understanding the model‚Äôs failure modes, quantifying the model‚Äôs uncertainty and teaching a model to be robust with respect to noise/adversarial perturbations. With the development of Data-Centric AI, I believe that many new and exciting things can be done in this area.\n","date":1645056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645056000,"objectID":"2e1225c6ffc0839c600af70c4057f45a","permalink":"https://jonathancrabbe.github.io/project/rml/","publishdate":"2022-02-17T00:00:00Z","relpermalink":"/project/rml/","section":"project","summary":"Making sure that machines are doing what they are supposed to do.","tags":["Machine learning","Deep learning","Robustness","Reliability","Safety"],"title":"Robust Machine Learning","type":"project"},{"authors":["Jonathan Crabb√©","Zhaozhi Qian","Fergus Imrie","Mihaela van der Schaar"],"categories":null,"content":"","date":1637107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637107200,"objectID":"6b58acdbae782e99fd5e825a6a7d833c","permalink":"https://jonathancrabbe.github.io/publication/simplex/","publishdate":"2021-11-17T00:00:00Z","relpermalink":"/publication/simplex/","section":"publication","summary":"We introduce SimplEx, a case-based reasoning explanation method that permits to decompose latent representations with a corpus of example.","tags":["Explainability","Interpretability","Corpus","Latent Representation","Feature Importance","Example Importance","Influential Examples","Case-Based Reasoning","Deep Learning"],"title":"Explaining Latent Representations with a Corpus of Examples","type":"publication"},{"authors":["Jonathan Crabb√©","Mihaela van der Schaar"],"categories":null,"content":"","date":1623196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623196800,"objectID":"b428aa361fdd98ce871ad7675c40fb49","permalink":"https://jonathancrabbe.github.io/publication/dynamask/","publishdate":"2021-06-09T00:00:00Z","relpermalink":"/publication/dynamask/","section":"publication","summary":"We introduce Dynamask, a perturbation-based feature importance method to explain the predictions of time series models.","tags":["Explainability","Interpretability","Time Series","Masks","Feature Importance","Machine Learning","Deep Learning"],"title":"Explaining Time Series Predictions with Dynamic Masks","type":"publication"},{"authors":["Jonathan Crabb√©","Yao Zhang","William R. Zame","Mihaela van der Schaar"],"categories":null,"content":"","date":1605571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605571200,"objectID":"0bcdc340004444872ecc162ff96ab139","permalink":"https://jonathancrabbe.github.io/publication/symbolic_pursuit/","publishdate":"2020-11-17T00:00:00Z","relpermalink":"/publication/symbolic_pursuit/","section":"publication","summary":"We introduce Symbolic Pursuit, a new method for symbolic regression based on Meijer G-functions and the projection pursuit algorithm.","tags":["Symbolic Regression","Interpretability","Meijer G-Function","Projection Pursuit","Machine Learning","Deep Learning"],"title":"Learning outside the black-box: the pursuit of interpretable models","type":"publication"},{"authors":null,"categories":null,"content":"How can we make a machine learning model convincing? If accuracy is undoubtedly necessary, it is rarely sufficient. Models such as neural networks typically involve millions of operations to turn their input data into a prediction. This complexity permits to accurately solve hard problems like computer vision and protein structure prediction. However, this accuracy comes at the expense of interpretability: these complex models appear as black boxes for human users. When models penetrate critical areas such as medicine, finance and the criminal justice system, their black-box nature appears as a major issue. An important question follows: is it possible to explain the predictions of complex machine-learning models?\nExplainable AI tackles this question by providing an interface between complex models and human users. To illustrate, let us consider the example of a medical machine learning model that recommends a treatment for a patient. By using post-hoc explainability, we can answer crucial questions such as ‚ÄúWhat part of this patient‚Äôs data motivates the model‚Äôs recommendation?‚Äù or ‚ÄúAre there similar patients previously seen by the model for which this treatment worked?‚Äù. In a setting where human knowledge is available (e.g. computer vision), this type of information is crucial to validate/debug the model. In a setting where little human knowledge (e.g. scientific discovery), this type of information permits to extract knowledge from the model.\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"07f8e974c92fc49679f5a434ce1d7cbe","permalink":"https://jonathancrabbe.github.io/project/xai/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/project/xai/","section":"project","summary":"Creating an interface between machine learning models and human beings.","tags":["Machine learning","Deep learning","Transparency","Explainability","Interpretability"],"title":"Explainable Artificial Intelligence","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://jonathancrabbe.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]