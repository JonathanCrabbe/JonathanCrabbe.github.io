<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Robustness | Jonathan Crabb√©</title>
    <link>https://jonathancrabbe.github.io/tag/robustness/</link>
      <atom:link href="https://jonathancrabbe.github.io/tag/robustness/index.xml" rel="self" type="application/rss+xml" />
    <description>Robustness</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 25 Oct 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jonathancrabbe.github.io/media/icon_hudf155d04df3b15b0b3080a45aba6eb0e_2672_512x512_fill_lanczos_center_3.png</url>
      <title>Robustness</title>
      <link>https://jonathancrabbe.github.io/tag/robustness/</link>
    </image>
    
    <item>
      <title>Robust multimodal models have outlier features and encode more concepts</title>
      <link>https://jonathancrabbe.github.io/publication/eri/</link>
      <pubDate>Wed, 25 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://jonathancrabbe.github.io/publication/eri/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance</title>
      <link>https://jonathancrabbe.github.io/publication/robustxai/</link>
      <pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://jonathancrabbe.github.io/publication/robustxai/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Joint Training of Deep Ensembles Fails Due to Learner Collusion</title>
      <link>https://jonathancrabbe.github.io/publication/joint_training/</link>
      <pubDate>Thu, 26 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://jonathancrabbe.github.io/publication/joint_training/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust Machine Learning</title>
      <link>https://jonathancrabbe.github.io/project/rml/</link>
      <pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://jonathancrabbe.github.io/project/rml/</guid>
      <description>&lt;p&gt;How can we make a machine learning model reliable? If generalization to unseen data is undoubtedly necessary, it is rarely sufficient. Models such as neural networks typically involve millions of operations to turn their input data into a prediction. This complexity permits to solve hard problems like computer vision and protein structure prediction. However, these complex models tend to exhibit unpredictable behaviours, such as the sensitivity to adversarial perturbations and spurious correlations . When models penetrate critical areas such as medicine, finance and the criminal justice system, this unpredictability is highly problematic.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Robust machine learning&lt;/em&gt; tackles this problem by providing tools to identify and fix the weaknesses of a model. This includes (but is not limited to) understanding the model&amp;rsquo;s failure modes, quantifying the model&amp;rsquo;s uncertainty and teaching a model to be robust with respect to noise/adversarial perturbations. With the development of Data-Centric AI, I believe that many new and exciting things can be done in this area.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
