<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine learning | Jonathan Crabbé</title>
    <link>https://jonathancrabbe.github.io/tag/machine-learning/</link>
      <atom:link href="https://jonathancrabbe.github.io/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 03 Mar 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jonathancrabbe.github.io/media/icon_hudf155d04df3b15b0b3080a45aba6eb0e_2672_512x512_fill_lanczos_center_3.png</url>
      <title>Machine learning</title>
      <link>https://jonathancrabbe.github.io/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>Representation Learning</title>
      <link>https://jonathancrabbe.github.io/project/repl/</link>
      <pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://jonathancrabbe.github.io/project/repl/</guid>
      <description>&lt;p&gt;How can we make sure that machines learn a good representation of the data? If labelling the data is a typical solution, it is often costly and time-consuming. Models such as neural networks typically require thousands to millions of examples to solve a task. Is it possible to learn good representations of the data without having to label each one of these examples?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Representation Learning&lt;/em&gt; provides interesting solutions through semi-supervised and self-supervised learning. With these techniques, it becomes increasingly realistic to solve complex tasks with few labelled examples. In some cases, we might want these representations to be understandable by human users. This induces a significant overlap with &lt;a href=&#34;https://jonathancrabbe.github.io/project/xai/&#34;&gt;Explainable Artificial Intelligence&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robust Machine Learning</title>
      <link>https://jonathancrabbe.github.io/project/rml/</link>
      <pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://jonathancrabbe.github.io/project/rml/</guid>
      <description>&lt;p&gt;How can we make a machine learning model reliable? If generalization to unseen data is undoubtedly necessary, it is rarely sufficient. Models such as neural networks typically involve millions of operations to turn their input data into a prediction. This complexity permits to solve hard problems like computer vision and protein structure prediction. However, these complex models tend to exhibit unpredictable behaviours, such as the sensitivity to adversarial perturbations and spurious correlations . When models penetrate critical areas such as medicine, finance and the criminal justice system, this unpredictability is highly problematic.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Robust machine learning&lt;/em&gt; tackles this problem by providing tools to identify and fix the weaknesses of a model. This includes (but is not limited to) understanding the model&amp;rsquo;s failure modes, quantifying the model&amp;rsquo;s uncertainty and teaching a model to be robust with respect to noise/adversarial perturbations. With the development of Data-Centric AI, I believe that many new and exciting things can be done in this area.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Explaining Time Series Predictions with Dynamic Masks</title>
      <link>https://jonathancrabbe.github.io/publication/dynamask/</link>
      <pubDate>Wed, 09 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://jonathancrabbe.github.io/publication/dynamask/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning outside the black-box: the pursuit of interpretable models</title>
      <link>https://jonathancrabbe.github.io/publication/symbolic_pursuit/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://jonathancrabbe.github.io/publication/symbolic_pursuit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interpretable Machine Learning</title>
      <link>https://jonathancrabbe.github.io/project/xai/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://jonathancrabbe.github.io/project/xai/</guid>
      <description>&lt;p&gt;How can we make a machine learning model convincing? If accuracy is undoubtedly necessary, it is rarely sufficient. Models such as neural networks typically involve millions of operations to turn their input data into a prediction. This complexity permits to accurately solve hard problems like computer vision and protein structure prediction. However, this accuracy comes at the expense of interpretability: these complex models appear as black boxes for human users. When models penetrate critical areas such as medicine, finance and the criminal justice system, their black-box nature appears as a major issue. An important question follows: is it possible to explain the predictions of complex machine-learning models?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Explainable AI&lt;/em&gt; tackles this question by providing an interface between complex models and human users. To illustrate, let us consider the example of a medical machine learning model that recommends a treatment for a patient. By using post-hoc explainability, we can answer crucial questions such as “What part of this patient’s data motivates the model’s recommendation?” or “Are there similar patients previously seen by the model for which this treatment worked?”. In a setting where human knowledge is available (e.g. computer vision), this type of information is crucial to validate/debug the model. In a setting where little human knowledge (e.g. scientific discovery), this type of information permits to extract knowledge from the model.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
